### 2.11 ubuntu以及本地的虚拟环境占用了自己较多内存

**关于远程服务器的尝试：**

1、想将我的本机Django项目上传到github中去，但是目前因为文件太大了（15GB）上传失败了。具体的流程与思路可以见chatgpt中的归档。（在autodl中租了一个，目前创建虚拟环境成功后，在尝试激活的时候出错了）

2、如何在远程服务器中新建一个虚拟环境来泡深度强化学习的代码。具体的流程也可以见chatgpt的归档。（目前尝试的时候有点小报错）



### 2.27（学习远程租用服务器）

1、今天尝试了连接服务器的一些操作以及，在服务器上安装一些东西。

2、其他时间，这几天陆陆续续的在跑视频生成相关的代码。想自己先租一个便宜的服务器来玩一玩。先跑通我的测试代码看看。

我的思路是，先想试一下利用远程服务器快速验证我编写的推理代码的方案。



### AVCap（准备论文代码所需环境）

1、查看github文档，发现应该是模型需要下载：pretrained_weights中的文件先下载放入

2、（CAV-MAE链接：https://github.com/YuanGongND/cav-mae）理解：

报错一：ffmpeg 未被正确安装

```
sudo apt update
sudo apt install ffmpeg -y
在ubuntu系统中的安装方式-》解决
```

报错二：TypeError: __init__() got an unexpected keyword argument 'qk_scale'

timm安装的版本太新了



### 3.1（研究论文代码）

1、给visual_input加个维度

**2、动了dataloader.py的48行，加了最后一个参数；   54行**

新增参数max_text_len=512

54行新增：self.max_text_len = max_text_len ##新增



### 3.2（尝试自己编写测试代码）

备注：自己编写的代码没成功，给删了。周报里有过程

1、运行run_avcap.py需要额外安装的一些库：

```
pycocotools==2.0.7

pycocoevalcap==1.2

boto3
```

**2、tokennizer仍然报错：（dataloader.py中也需要相应改动为本地加载）**
关键原因应该是：缺少 special_tokens_map.json
1)vim special_tokens_map.json
2)按i进入编辑模式：粘贴以下：

```
{
  "unk_token": "[UNK]",
  "sep_token": "[SEP]",
  "pad_token": "[PAD]",
  "cls_token": "[CLS]",
  "mask_token": "[MASK]"
}
3）Esc
4）：wq
```

3、尝试执行代码的推理模式：指令如下：（注意我premodel中的cav-scale++是我的lastest——model的改名）
（运行以下的代码之后，就可以正常的开始我的这份代码的训练过程了）

```
python run_avcap.py \
    --gpu 0 \
    --data-val dataset/test.json \
    --ann_file dataset/test_coco.json \
    --torch2iid_file dataset/torch2iid.json \
    --pretrained_model models/model_latest_ft.pth \
    --mode captioning \
    --batch-size 1 \
    --num-workers 4 \
    --distributed False \
    --no_wandb \
    --save_path results
```



### 3.3（深入研究模型代码，编写相应测试代码）

1、两个模型相对应

model 对于最新的模型

av_model对应cva-mae-scale++模型（有moudle.的前缀）

**2、改了trainer.py的54-59行、79行、220行（去掉了module）**

```
python mytest.py --mode captioning \
--av_pretrain_path pretrained_weights/hhh.pth \
--text_pretrain_path pretrained_weights/cav-mae-scale++.pth \
--data-test dataset/test.json
```



### 3.5（减少测试数据量，加快代码迭代）

1、**`test.json` 是“数据地图”，通过 `audio_path` 和 `video_path` 指向具体的音频和视频文件。**

test.json文件里面存了944个字典。

每个字典：

```
"audiocap_id": "103549",
    "youtube_id": "yVVLq4ao1Ck",
    "start_time": "20",
    "caption": "Constant rattling noise and sharp vibrations",
    "labels": "/m/09x0r,/m/0llzx",
    "audio_path": "yVVLq4ao1Ck_20000_30000.wav",
    "video_path": "yVVLq4ao1Ck_20000_30000.jpg",
    "torch_id": 0
```

每个frame里面有944张图片

wavforms里面也刚好有944个音频

每个frame的照片连起来就是一个视频

2、再查一查其他的json文件，看看是否也有对应

```
test_coco.json文件内容：
{
  "id": "7fmOlUlwoNg",
  "labels": "/m/09x0r,/m/0ytgt,/t/dd00125",
  "audio_path": "7fmOlUlwoNg_20000_30000.wav",
  "video_path": "7fmOlUlwoNg_20000_30000.jpg"
}
```

**3、成功降完数据，需要尝试再跑一跑了**

解决报错/去掉coco

**服务器未安装java报错**

```
sudo apt install openjdk-17-jdk  # 安装 OpenJDK 17
```

4、重要：要么降数据，要么降时间，要么快点吧生成结果得到

18：00-18：14（代码全部跑完）

**5、目前为止可以跑通的代码**

1）一个是mytest（这种关键代码还是得靠人去编写）

2）二个是run_avcap.py（放置好合适的pth或者pt模型后即可）

6、字典好像是按顺序对应的，所以我随机生成的50张图片的识别率才那么低

```
不使用coco评估时，直接保存模型输出结果的代码：
"""   
        ############结果保存############
        os.makedirs(args.result_save_path, exist_ok=True)
        json_save_path = os.path.join(args.result_save_path, f"epoch{1}_coco.json")

        # 保存结果到 JSON 文件
        with open(json_save_path, 'w') as f:
            json.dump(result_json, f)

        print(f"结果已保存到: {json_save_path}")
"""
```

22：29

查看coco评估时输出文件：

```
cp /data/exp/output_try34/result/epoch1_coco.json /root/AVCap/
cp /data/exp/output_try34/result/log.txt /root/AVCap/


cd /data/exp/output_try18/result/
ls
```



### 3.6（继续调整模型文件，提高生成质量）

1、调整某些参数可以降低显存：

1）：降低 --batch-size（最直接有效）

2）：降低帧数：--num_frames）（会影响准确性）

3）：降低数据加载工作线程数--num-workers（可以适当降低）

2、今天下午做的

理解了github给的最新模型结构。并且加载该模型跑通了

3、目前问题：

提高模型生成质量

**论文中信息：**

```
1、音频视频编码器使用CAV-MAE-scale++，训练阶段使用的git-base文本解码器初始化，遵循BERTB，有12层
2、训练期间，保持音频视频编码器的更新。冻结文本解码器
3、视听编码器架构中使用11层
4、数据集中以音频为中心
```

### 3.7（用自己编写的测试代码成功跑出论文预期效果）

将model_latest_ft.pth**拆开分别替换音频编码器和文本文本解码器**：可以有生成描述，但是效果很差。

将model_latest_ft.pth**拆开只替换音频编码器而冻结文本文本解码器git-base**：生成的是乱码。

将model_latest_ft.pth**整体替换音频编码器和替换文本文本解码器git-base**：可以有生成描述，但是效果很差。

latest完整+latest拆分：差

latest拆分+latest完整：差

2、昨晚用将model_latest_ft.pth**拆开分别替换音频编码器和文本文本解码器**测试了944个数据：效果还是一样的差：

结论：和测试数据量无关，想快速迭代的话，数据量还可以降低

3、

检查哪些参数未正确加载

**4、感谢gpt的协助以及自己的坚持，跑出论文结果了**

```
今日可运行语句：
python mytest.py --mode captioning \
--av_pretrain_path pretrained_weights/cav-mae-scale-new.pth \
--text_pretrain_path pretrained_weights/model_latest_ft.pth \
--data-test dataset/test_reduced_1.json

dataset/audiocaps/test_1
```

**运行指南：**直接运行mytest.py文件，缺少什么文件就下载什么。运行语句如上

### AVCap结束（调整原始数据处理）

1、统一自己的数据的python脚本需要安装的库：

```
pip install opencv-python pydub pillow
```



### 3.20:开始复现ReliTalk代码

**一、主要进行代码运行的环境搭建**

conda create --name deca-env python=3.8

**a、deca-env虚拟环境的搭建以及包的安装：**

1、抱错：bash: rsync: command not found

安装就行：

```
sudo apt update
sudo apt install rsync -y
```

2、运行 preprocess_Obama.sh报错了，应该是windows和linux的格式问题

sudo apt update && sudo apt install dos2unix -y

dos2unix preprocess_Obama.sh

后可以运行bash preprocess_Obama.sh了，但是缺少安装一些库：（按照顺序安装即可）

```
1、安装cuda、torch、torchvision以及pytorch3d（一共需要3h）
pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113

pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py38_cu113_pyt1110/download.html

数据预处理部分（使用GPU来加速）：pip install nvidia-tensorrt

cv2:conda install -n deca-env opencv -y

pip install scipy

pip install scikit-image

pip install kornia

pip install yacs

pip install face_alignment==1.3.5

pip install chumpy（报错，解决如下）
```

```
在服务器中找到chumpy的安装路径并且修改代码：
pip show chumpy

vim __init__.py
按 i 进入编辑模式，修改 from numpy import bool, int, float, complex, object, unicode, str, nan, inf 为：

import numpy as np
bool = np.bool_
int = np.int_
float = np.float_
complex = np.complex_
object = np.object_
str = np.str_
nan = np.nan
inf = np.inf

然后按 Esc，输入 :wq 保存并退出。
```

```
pip install face_alignment

pip install ninja

pip install torchfile

安装gcc-7,并且设置gcc优先级
sudo update-alternatives --config gcc
sudo update-alternatives --config g++

2、报错ImportError: /lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.29'
直接用已经有的lilbstdc去直接替换：
sudo cp /usr/local/miniconda3/pkgs/libstdcxx-ng-11.2.0-h1234567_1/lib/libstdc++.so.6 /lib/x86_64-linux-gnu/libstdc++.so.6
sudo ldconfig

```



**b、补充在ReliTalk虚拟环境里面需要安装的包：**

```
开始在Relialk环境里面安装包（之前都是在deca-env里面安装的包）
1）pip install pyhocon
2）Relialk里面也要有以下安装：
pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113
3）pip install plotly
pip install scikit-image
pip instsll trimesh
pip install opencv-python
pip install wandb

（pytorch3d的依赖提前安装：pip install fvcore）（检查语句：conda list pytorch3d）
pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py38_cu113_pyt1110/download.html

pip install chumpy后出现和再deca-env中一样的报错
pip install imageio==2.15.0
pip install matplotlib
pip install --no-cache-dir facenet_pytorch --no-deps或者：pip install --no-cache-dir facenet_pytorch==2.5.2 --no-deps来避免将我之前安装的torch、torchvision给替换掉
```

**禁用wandb的方法：**

```
修改了train.py中432到435代码，以及40行以下的一些代码
同样，修改了train_relight.py中的一些代码
```

**二、论文代码的复现**

**a、降低训练次数以及测试、训练数据量，快速检验代码运行结果**

1、训练批次从120降低到10

2、数据也进行了大降，估计还是得要花挺多时间，训练

3、关于：rough normal我并没有训练，也跑通了eval（好像进行了数据预处理后，就自带了模型参数（有待考究））

1）Generate rough normal

```
python scripts/exp_runner.py --conf ./confs/IMavatar_supervised_Obama_test.conf --is_eval --checkpoint [epoch1]
```

生成内容位置：

data/experiments/Obama/IMavatar/Obama_train/eval/**Obama_train**/epoch_0里面所有（所有训练帧，每帧80秒）

data/experiments/Obama/IMavatar/Obama_train/eval/**Obama_eval**/epoch_0里面所有（所有测试帧，每帧80秒）



**b、将论文代码初步跑通**

1、目前情况：

1）Train for rough normal经测试，12GB的3060显卡可以跑动，但是要花费2-3天的时间训练，暂未执行

目前将训练批次eporch从120降低到了10

7天

2）Generate rough normal 可以并且执行了（测试+训练图片帧，每一帧要花费80秒）

**花费时间：**（测试+训练图片共400张）8.8小时

3）Train for reflectance decomposition 目前一跑就报显存了，需要找找原因。少量输出如下：

```
Saving image 1 into ../data/experiments/Obama/IMavatar/Obama_train/eval/Obama_eval/epoch_0
```

通过减少batch_size来试一试

将batch_size从8降低到2成功解决

**花费时间：**（256张训练图片）34分钟

正常输出结果：

```
IMavatar [0] (0/256): loss: 

IMavatar [10] (250/256): loss: 0.43218424916267395 rgb_loss: 
Epoch time: 188.6292428970337
```

4）Inference for relighting 跑通了，并且执行速度非常的快。输出结果如下：

```
Saving image 7 into ../data/experiments/Obama/IMavatar/Obama_train/eval/Obama_eval/epoch_0
```

**花费时间：**（129张测试图片）34秒



**三、存在问题以及展望**

**1、难点：**第一个繁琐点是环境配置，该代码环境使用的库以及工具较为丰富，配置环境时需要花较长时间和耐心。（已经解决）

**2、存在问题：**代码从数据预处理到生成结果的时间非常长，在我降低代码部分训练批次以及大幅度降低视频时间的情况下，仍然花费了2.5天的运行时间。（前期环境配置与安装也额外花费6-7小时）

**3、展望：**

1）下一步打算再次深入的阅读野外光照这个代码的相应论文，看看是否能降低数据从输入再到生成预期结果所需要的时间。（由于代码提供的初始视频时间更长，可能需要一个星期甚至更长的时间来生成相应的结果，打算之后等设备到了，再在设备上进行长时间运行）

2）学习如何将复现的论文代码功能集成至之前开发的系统里面，尝试将上一周成功复现的视频标题的功能集成至系统里面。





2、目前全部可以跑通了，但是时间花费还是较长

3、先不动代码了，将租的服务器先关机了，先再看看论文



3.20

### 师姐介绍了一下她现在做的方向

，说上手应该会比较的容易，可以考虑看她这方面的论文



### 3.21

1、再次看完论文

第四实验部分+第五结论部分对实验较为有用

2、github中Relight四个阶段的逻辑关系

```
1️⃣ 训练粗糙法线估计（训练阶段）
目的： 估计人物面部的几何结构（法线 Normal），让后续光照计算有合理的形状信息。
📌 输入： 训练视频中的人脸帧
📌 输出： 法线图（Normal Map）
2️⃣ 生成粗糙法线（推理阶段）
📌 目的： 从训练好的法线估计模型生成法线图，作为光照分解的输入
📌 输入： 训练好的模型（来自第1步） + 人物视频
📌 输出： 估计的法线图（Normal Map）
3️⃣ 训练反射率分解（训练阶段）
📌 目的： 让模型学习如何从图像中拆解出光照信息，得到：
	反射率（Albedo）：即物体本身的颜色（去除阴影影响）
	光照（Lighting）：即环境光照的影响
	阴影（Shading）：即光照在物体表面造成的明暗变化
📌 输入： 训练数据 + 估计的法线图（来自第2步）
📌 输出： 训练好的反射率分解模型
4️⃣ 进行重光照渲染（推理阶段）
📌 目的： 让人物在新的光照条件下保持逼真的视觉效果
📌 输入： 训练好的反射率分解模型（来自第3步）+ 视频帧 + 目标光照
📌 输出： 重新渲染的视频（带有新光照）
```

### 3.22

1、**云服务器技巧**（要是没显卡了。可以实例管理->创建备份镜像->重新创建一个实例的时候选择所备份的镜像）

2、试一下将上一周成功复现的视频标题的功能集成至系统里面。（还并未开始，往深处走）

3、再来跑一跑

1）数据预处理部分：

**花费时间：**18：50-20：50（2小时）

可以安装（使用GPU）：pip install nvidia-tensorrt

数据预处理部分的功能流程：

```
功能流程
✅ 数据预处理

视频裁剪、缩放 (ffmpeg)

背景/前景分割 (MODNet)

✅ 特征提取 3. 逐帧保存图像 4. 关键点检测 (keypoint_detector.py) 5. 虹膜分割 (iris.py)

✅ FLAME 参数估计 6. 3D 人脸建模 (DECA) 7. FLAME 参数优化 (optimize.py)

✅ 语义分割 8. 面部解析 (face-parsing.PyTorch)
```

生成文件夹内容解释：ReliTalk\data\datasets\Obama\Obama\Obama_eval

```
1、image/
存放从视频中提取的帧图像，用于后续处理（关键点检测、3D建模等）
2、mask/
存放 MODNet 或类似模型生成的前景遮罩（mask）
3、semantic/
存放面部语义分割（face parsing）结果，即对脸部区域进行类别分割（例如眼睛、鼻子、嘴巴等）
4、semantic_color/
语义分割结果的彩色版本，与 semantic/ 对应，但颜色编码不同，使可视化更直观
5、code.json/（DECA 计算的 FLAME 参数，包括形状、表情、姿态、相机参）
{
  "shape": [...],  // 人脸形状参数（描述静态几何特征）决定面部几何形状。
  "exp": [...],    // 表情参数（控制动态表情变化）决定面部几何形状。
  "pose": [...],   // 头部姿态参数（旋转角度等）控制头部方向，影响3D人脸渲染。
  "cam": [...]     // 相机参数（透视投影信息）影响最终的 3D 投影到 2D 图像的效果。
}
6、flame_params.json（FLAME 优化参数，包含关键点、变换矩阵、人脸检测框等）
{
  "file_path": "./image/2",  // 处理的图片路径
  "world_mat": [...],       // 3D世界坐标到相机坐标的变换矩阵
  "expression": [...],      // FLAME 模型的表情参数
  "pose": [...],            // 头部姿态参数
  "bbox": [...],            // 人脸检测到的边界框（bounding box）
  "flame_keypoints": [...]  // FLAME 生成的3D关键点
}
7、iris.json（虹膜检测结果，可能是眼睛中心点或检测框）
{
  "901.png": [152.91, 124.63, 105.16, 120.85],
  "902.png": [153.53, 125.16, 105.49, 121.26]
}
8、keypoint.json（关键点包括：轮廓、眼睛、鼻子、嘴巴）
{
  "902.png": [[66.0, 122.0], [66.0, 138.0], ..., [118.0, 184.0]]
}

6、optimize.vis.jpg/
optimize.py 生成的优化结果可视化。展示优化前后FLAME模型的对比，或是3D拟合效果。
```



2）运行Train for rough normal

报了有关git的错：执行以下解决：

```
git init
git add .
git config --global user.name "zhang" 
git config --global user.email "2277583889@qq.com" 
git commit -m "Initial commit"
```

**禁用了wandb，并将结果打印，以及放入data的jsonfile里面**

**花费时间：**训练一个批次43分钟，共120个批次需要83小时

生成结果文件夹解释：

```
1、experiments/Obama/IMavatar/Obama_train/中的eval/Obama_eval/rendering/
都是随着训练批次eporch递增的png图片，其中的蓝紫色调的法线图是粗糙法线估计结果
2、experiments/Obama/IMavatar/Obama_train/中的train/目录：
存放训练好的模型参数、训练配置文件
```

注：在VScode里面使用tumx来避免应为网络问题导致VScode断开，从而终止正在运行的进程

```
按 Ctrl + b 然后 [ 进入滚动模式（Copy Mode）
启动：tmux new -s mysession
断开：按 Ctrl + b 然后 d 断开 tmux，此时任务仍然在后台运行。
恢复：tmux attach -t mysession
```



### 3.23

1、ReliTalk的实验继续跑，我这边尝试一下部署到我的系统

3.24开始将AVCap集成至系统

```
启动Django服务器的指令：
python manage.py runserver

启动后访问的网址：
http://127.0.0.1:8000/Space_Upload_1/
```

主要问题：

```
现在我将我的AVCap文件已经放入Django_v3的文件夹下了，但是我还是有许多的问题想要问一下你，这些问题可能很庞大，但是我想一步一步的来解决以至完成。
首先是第一个问题，就是我的AVCap文件之前的运行代码是：
python mytest.py --mode captioning \
--av_pretrain_path pretrained_weights/cav-mae-scale-new.pth \
--text_pretrain_path pretrained_weights/model_latest_ft.pth \
--data-test dataset/test_reduced_1.json
我是不是得改，使我的mytest.py一调用就可以执行？

第二个问题就是关于数据的问题这个Django前任开发人员在前端的功能是：点击选择文件，即可从本地上传一个视频（当然这个我可以复用），之后点击开始处理，就会对上传的视频进行处理了。然而我的AVCap之前对数据的处理是：先将视频文件放入AVCap->dataset->audiocaps->test里面，再经过该目录下的py文件将其分为帧和音频。同时还得再AVCap->dataset->audiocaps下同步一些json文件。

所以我觉得以上两个难点是我将复现的github代码集成至系统中的难点。因为之前的代码复现的话，例如：数据、json文件的同步、数据预处理、以及带参数的python执行命令自己都可以分步骤的去完成，而现在要集成至系统中的话我会觉得较为的复杂。请你详细的告我我该怎么做，要是工程量很大的话，我想分多天来完成
```

gpt方案：三步走：

```
1、第一步：修改 mytest.py，使其可以直接被 Django 视图调用
目标：把 mytest.py 里的逻辑封装成一个 Python 函数，让 Django 视图可以调用，而不是只能用 python mytest.py --xxx 命令执行。

改造方式：

提取 mytest.py 的核心推理逻辑，封装成一个 run_avcap_inference() 函数。

支持 Django 直接调用这个函数，传入文件路径等参数，而不是通过命令行参数。

2、第二步：在 Django 视图 views.py 里调用这个推理代码
现在我们修改 views.py，让前端上传视频后，Django 能够调用 run_avcap_inference() 进行推理。

步骤：

让前端上传视频

Django 处理上传文件，传递给 AVCap

调用 run_avcap_inference() 进行推理

返回推理结果

3、第三步：改造 AVCap 数据预处理，让 Django 自动完成数据预处理
现在，Django 已经可以传递视频路径，但是 AVCap 仍然需要手动放入 dataset/audiocaps/test/ 目录，并运行预处理代码。我们需要 自动完成这些步骤。

你需要：

自动把上传的视频存放到 AVCap/dataset/audiocaps/test/ 目录

自动运行 AVCap 预处理代码，把视频转换为帧和音频

自动同步 JSON 文件

然后再调用 run_avcap_inference() 进行推理

如果你的 AVCap 预处理代码在 AVCap/dataset/audiocaps/preprocess.py，你可以改成 一个可调用的 Python 函数：
```

Django系统改动部分：

```
mytest文件（去参数）
```

关于环境问题考虑是每个项目启用独立的虚拟环境

报显存了（那就先不部署了）

### 3.24：ReliTalk暂停

在第一个阶段训练后运行第二步加载法线，报显存了

但是不运行第一步，直接运行第二步时，不会报显存







### 4.14

linux系统下运行的代码：

✅ **迁移完全可行**。
 你需要的只是：适配路径 + 改写部分 shell 指令 + 确保环境完整。
 代码是 Python 写的，本质是跨平台的，**跑模型跟操作系统无关，只跟环境有关**。

如果你后续要我帮你把 `mv` 改成 Python 脚本、或者把 `exp_runner_relight.py` 整合成 Django 调用模块，我可以一步步帮你改～

4.15的尝试

因为没有显卡，所以依然不运行第一步，直接继续尝试运行第二步，

1）并且换个清晰的背景试试（直接初始数据自己找一个）

2）并且尝试固定一下光照的方向（这个需要研究代码看看）

3）再次测量一下psnr

4）其实最后生成的效果还可以，只是我的视频从帧转化为视频时有点问题

4.19

尝试重新将图片转化成帧：

1、验证：

其实最后生成的效果还可以，只是我的视频从帧转化为视频时有点问题

结果：还是有问题，不是gpt代码生成问题





4.19

### ✅ **这两天建议重点做的核心工作**

#### 1️⃣ **搞清楚系统的整体结构和野外光照模块能插入的接口位置**

- 梳理项目目录结构，理清：
  - 系统从前端点击按钮后，最终会调用到哪个 `views.py` 函数；
  - 视频是如何上传、存到哪、怎么传给后端处理的；
  - 原有的处理函数是如何组织的（尤其是路径、模型调用位置等）。
- 找出你“野外光照”模型可以接入的位置，写下调用流程草图（比如从前端上传 -> views.py 处理 -> 调用你写的 `relighting.py` -> 返回结果）。

#### 2️⃣ **初步写一个后端 demo 来跑通你复现的光照算法**

- 准备一个最小 demo：
  - 输入一个视频；
  - 模拟光照变化（哪怕是用你模型的 test 函数跑一帧）；
  - 输出视频或图片结果（存在 static/ 里供前端展示）；
  - 计算并返回 PSNR 值（哪怕先写死数值也没关系）。
- 这样你后续只要和系统接口对接即可，逻辑已经打通。

#### 3️⃣ **设计一个“页面长什么样”的简单草图并截图发给组员或老师确认**

- 用手画或用 PPT 画一个简图，比如：
  - 上传原视频区域；
  - “开始处理”按钮；
  - 视频展示框；
  - PSNR 值显示区域。
- 目的不是让你写前端，而是让大家达成共识：“我们这个子页面大概长这样，功能这么交互。”

------

### 💡 总结一句话：

> **你这两天只要搞清楚系统结构，定位模块接入点，并写出一个跑得通的后端测试代码流程，顺便画个交互草图确认方向，这阶段就稳了。**



### 4.19

#### 1、预期页面的设计：

1）我的野外光照结果就两个指标：
展示生成的重光照效果+生成视频的PSNR指标

2）**预期展示效果：**

我的视频上传部分和之前的已有的前端完全一样。不做改动。

具体描述如下：

预期效果图如下：



3）**预期展示效果：**

我生成的东西就两个：带有重新光照的视频效果+生成视频的PSNR指标

也基本沿用原来的框架，右边的指标改为自己的PSNR，左边展示的部分改为生成视频+原始视频，也还可以加上中间生成结果，让其看上去不至于那么的单调。